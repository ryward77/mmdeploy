class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)
  def forward1(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)
  def forward2(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward3(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward4(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)
  def forward5(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)
  def forward6(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward7(self: __torch__.torch.nn.modules.activation.___torch_mangle_43.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
